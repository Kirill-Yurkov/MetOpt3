{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3601fd2",
   "metadata": {},
   "source": [
    "#### **Цель работы:**\n",
    "Изучить и провести сравнительный анализ различных методов оптимизации для нахождения приближения к точке минимума нелинейной функции. Оценить эффективности методов с точки зрения числа итераций, вычислительной сложности и качества получаемого решения.\n",
    "\n",
    "#### **Задачи:**\n",
    "1. Реализовать метод градиентного спуска для нахождения приближения к точке минимума функции.\n",
    "2. Применить метод Ньютона для нахождения приближения к точке минимума.\n",
    "3. Реализовать модифицированный метод Ньютона, в котором матрица Гессе вычисляется однократно в начальной точке. Проанализировать сходимость метода.\n",
    "4. Применить метод Бройдена с выбором шага спуска при помощи метода золотого сечения.\n",
    "5. Реализовать метод Бройдена с выбором шага спуска на основе условий Армихо.\n",
    "6. Визуализировать процесс оптимизации для каждого метода: построить графики убывания функции по итерациям.\n",
    "7. Оценить и сравнить вычислительную сложность рассмотренных методов, выбрать наиболее эффективный алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc6ee0",
   "metadata": {},
   "source": [
    "В данной работе, как и в первой, будем исслодвать методы на примере функции Химмельблау, которая определена как: \n",
    "$$ \n",
    "f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 \n",
    "$$. \n",
    "Данная функция принадлежит классу гладких функций $ C^{\\infty} $ и является многомодальной, то есть имеет несколько локальных минимумов, а именно: \n",
    "$$(3.0;2.0) \\quad (-2.805;3.131) \\quad (-3.779;-3.283) \\quad (3.584;-1.848).$$\n",
    "\n",
    "\n",
    "Градиент функции определяется как вектор, составленный из частных производных по переменным x и y. Для нашей функции:\n",
    "    $$\\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7)$$\n",
    "    $$\\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beabeac",
   "metadata": {},
   "source": [
    "Импортируйте че вам там надо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45310b8",
   "metadata": {},
   "source": [
    "#### **Ход работы:**\n",
    "### Задание 1. Метод градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42030d0d",
   "metadata": {},
   "source": [
    "Этот итерационный метод минимизации функции был нами подробно рассмотрен в ЛР №1, поэтому детально на нем останавливаться не будем. Следует отметить, что он основан на движении в направлении антиградиента функции.\n",
    "\n",
    "\n",
    "Идея метода заключается в том, что на каждой итерации обновляется текущая точка $x_k$ по формуле:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha_k \\nabla f(x_k),\n",
    "$$\n",
    "\n",
    "где\n",
    "- $\\nabla f(x_k)$ — градиент функции в точке $x_k$,\n",
    "- $\\alpha_k > 0$ — шаг (скорость обучения), фиксированный или динамический."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c36ffa",
   "metadata": {},
   "source": [
    "Сюда реализацию алгоритма и какой мы берем шаг "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7e5ed",
   "metadata": {},
   "source": [
    "#### **Критерий остановки**\n",
    "Итерации продолжаются до тех пор, пока $ \\|\\nabla f(x_k)\\|_2 $ не станет меньше заданной точности. В нашем случае:\n",
    "$$\n",
    " \\|\\nabla f(x_k)\\|_2 \\leq 0.0001\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7c506",
   "metadata": {},
   "source": [
    "#### **Вычислительная сложность**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac04bd7",
   "metadata": {},
   "source": [
    "### Задание 2. Метод Ньютона.\n",
    "Это итерационный метод минимизации функции, который использует информацию о первой и второй производных функции (градиент и матрицу Гессе). Он находит приближение к точке минимума функции, используя квадратичную аппроксимацию целевой функции.\n",
    "\n",
    "На каждой итерации точка $x_k$ обновляется по формуле:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k),\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\nabla f(x_k)$ — градиент функции в точке $x_k$,\n",
    "- $\\nabla^2 f(x_k)$ — матрица Гессе,\n",
    "- $[\\nabla^2 f(x_k)]^{-1}$ — обратная матрица Гессе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e71410",
   "metadata": {},
   "source": [
    "#### **Условия сходимости**\n",
    "Для успешного применения метода Ньютона необходимо выполнение следующих условий:\n",
    "1. Гладкость функции:\n",
    "- Функция должна быть дважды непрерывно дифференцируемой. \n",
    "- Матрица Гессе должна существовать и быть невырожденной $\\det(\\nabla^2 f(x)) \\neq 0  $\n",
    "2. Начальное приближение $x_{0}$ должно быть достаточно близко к точке минимума. \n",
    "3. Если функция строго выпуклая ($\\nabla^2 f(x) > 0$), то метод Ньютона сходится к глобальному минимуму. \n",
    "4. При соблюдении условий метод сходится квадратично:$\\|x_{k+1} - x_{min}\\| \\leq C \\|x_k - x_{min}\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27aa84",
   "metadata": {},
   "source": [
    "### Задание 3. Модифицированный метод Ньютона.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23ab57",
   "metadata": {},
   "source": [
    "Это упрощённая версия классического метода Ньютона, в которой матрица Гессе вычисляется только один раз в начальной точке, что позволяет снизить вычислительные затраты, так как на каждой итерации не требуется пересчитывать и обращать матрицу Гессе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddbcfb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Метод использует локальную квадратичную аппроксимацию функции, но матрица Гессе фиксируется на начальной точке $x_0$. Таким образом, направление спуска определяется как:\n",
    "\n",
    "$$\n",
    "p_k = -[\\nabla^2 f(x_0)]^{-1} \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "Это означает, что направление спуска зависит только от градиента в текущей точке $x_{k}$, а кривизна функции учитывается через фиксированную матрицу Гессе $\\nabla^2 f(x_0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f736bf9",
   "metadata": {},
   "source": [
    "Обновление функции происходит следующим образом:\n",
    "$$\n",
    "x_{k+1} = x_k + p_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49015542",
   "metadata": {},
   "source": [
    "#### **Критерий остановки**\n",
    "Аналогично предыдущим методам??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80678a",
   "metadata": {},
   "source": [
    "#### **Сходимость**\n",
    "Глобально условия сходимости такие же как и в классическом методе Ньюттона, однако скорость сходимости может быть ниже, особенно если матрица Гессе существенно меняется вдоль траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfdf9c",
   "metadata": {},
   "source": [
    "#### Что-то умное для сравнения\n",
    "Модифицированный метод Ньютона является компромиссом между классическим методом Ньютона и градиентным спуском. Он снижает вычислительные затраты за счёт однократного обращения матрицы Гессе, но его сходимость может быть менее эффективной для задач с сильно меняющейся кривизной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e695fa4",
   "metadata": {},
   "source": [
    "### Задание 3. Метод Бройдена. \n",
    "Это квазиньютоновский метод минимизации функции, который аппроксимирует матрицу Гессе или её обратную. В отличие от метода Ньютона, здесь не требуется явно вычислять вторые производные, что значительно снижает вычислительные затраты. \n",
    "\n",
    "\n",
    "На каждой итерации точка $x_k$ обновляется по формуле:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - B_k^{-1} \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\nabla f(x_k)$ — градиент в точке $x_k$,\n",
    "- $B_k$ — аппроксимация матрицы Гессе (или ее обратной)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d88efc",
   "metadata": {},
   "source": [
    "Матрица $B_k$ обновляется по формуле:\n",
    "\n",
    "$$\n",
    "B_{k+1} = B_k + \\frac{(y_k - B_k s_k) s_k^\\top}{s_k^\\top s_k}\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ — изменение градиента,\n",
    "- $s_k = x_{k+1} - x_k$ — изменение аргумента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0df00",
   "metadata": {},
   "source": [
    "Алгоритм выполняет итерации до достижения предельного числа шагов. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625f522",
   "metadata": {},
   "source": [
    "#### **Выбор шага спуска.**\n",
    "#### 3.1. Метод золотого сечения.\n",
    "Задается интервал $[a, b]$. Далее вычислияются точки деления:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1 &= b - \\phi(b-a), \\\\\n",
    "x_2 &= a + \\phi(b-a),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "которые делят интервал на две части в пропорции золотого сечения ($\\phi = \\frac{\\sqrt{5}-1}{2} \\approx 0.618$). Далее будем вычислять значения функции в этих двух точках внутри интервала. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047664d",
   "metadata": {},
   "source": [
    "Затем будем сужать интервал, сохраняя пропорцию золотого сечения, пока не достигнем заданной точности. Сужение происходит по следующему принципу:\n",
    "\n",
    "- если $f(x_1) < f(x_2)$, то новый интервал $[a, x_2]$\n",
    "- если $f(x_1) \\geq f(x_2)$, то новый интервал $[x_1, b]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e27268",
   "metadata": {},
   "source": [
    "Оптимальный шаг находим следующим образом:\n",
    "$$\n",
    "\\alpha_k = \\mathop{\\mathrm{arg\\,min}}\\limits_{\\alpha > 0} f(x_k + \\alpha p_k)\n",
    "$$\n",
    "\n",
    "\n",
    "Шаг спуска выбирается численно, что обеспечивает более точное движение вдоль направления спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1140be",
   "metadata": {},
   "source": [
    "#### 3.2. Условия Армихо.\n",
    "Эти условия используются для выбора шага, который обеспечивает достаточное уменьшение функции и предотвращает слишком маленькие шаги.\n",
    "- Первое условие (достаточное уменьшение).\n",
    "\n",
    "Шаг $\\alpha_k$ должен удовлетворять неравенству:\n",
    "\n",
    "$$\n",
    "f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n",
    "$$\n",
    "\n",
    "где $0 < c_1 < 1$ — параметр, определяющий допустимое уменьшение функции (обычно $c_1 \\approx 10^{-4}$).\n",
    "- Второе условие (правило кривизны).\n",
    "$$\n",
    "\\nabla f(x_k + \\alpha_k p_k)^\\top p_k \\geq c_2 \\nabla f(x_k)^\\top p_k,\n",
    "$$\n",
    "\n",
    "где $c_1 < c_2 < 1$ — параметр кривизны (обычно $c_2 \\approx 0.9$). Это правило помогает избежать слишком маленьких шагов, которые могут замедлить сходимость.\n",
    "\n",
    "\n",
    "Параметры неточного поиска подберем самостоятельно, экспериментальным путем. Начнем со значения $\\alpha_k = 1$. Затем будем проверять указанные условия Армихо. Если они не выполняются то будем уменьшать шаг следующим образом:\n",
    "$$\n",
    "\\alpha_k = \\tau \\alpha_k, \\quad \\text{где } \\tau \\in (0,1) \\text{ — коэффициент уменьшения}.\n",
    "$$\n",
    "\n",
    "\n",
    "Условия Армихо гарантируют, что значение функции на каждой итерации уменьшается, что делает метод более устойчивым.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131c26d",
   "metadata": {},
   "source": [
    "### Сравнение методов\n",
    "итерации, вычислительная нагрузка и почему так"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
