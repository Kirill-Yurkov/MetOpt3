{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3601fd2",
   "metadata": {},
   "source": [
    "#### **Цель работы:**\n",
    "Изучить и провести сравнительный анализ различных методов оптимизации для нахождения приближения к точке минимума нелинейной функции. Оценить эффективности методов с точки зрения числа итераций, вычислительной сложности и качества получаемого решения.\n",
    "\n",
    "#### **Задачи:**\n",
    "1. Реализовать метод градиентного спуска для нахождения приближения к точке минимума функции.\n",
    "2. Применить метод Ньютона для нахождения приближения к точке минимума.\n",
    "3. Реализовать модифицированный метод Ньютона, в котором матрица Гессе вычисляется однократно в начальной точке. Проанализировать сходимость метода.\n",
    "4. Применить метод Бройдена с выбором шага спуска при помощи метода золотого сечения.\n",
    "5. Реализовать метод Бройдена с выбором шага спуска на основе условий Армихо.\n",
    "6. Визуализировать процесс оптимизации для каждого метода: построить графики убывания функции по итерациям.\n",
    "7. Оценить и сравнить вычислительную сложность рассмотренных методов, выбрать наиболее эффективный алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beabeac",
   "metadata": {},
   "source": [
    "Импортируйте че вам там надо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45310b8",
   "metadata": {},
   "source": [
    "#### **Ход работы:**\n",
    "### Задание 1. Метод градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42030d0d",
   "metadata": {},
   "source": [
    "Этот итерационный метод минимизации функции был нами подробно рассмотрен в ЛР №1, поэтому детально на нем останавливаться не будем. Следует отметить, что он основан на движении в направлении антиградиента функции.\n",
    "\n",
    "\n",
    "Идея метода заключается в том, что на каждой итерации обновляется текущая точка $x_k$ по формуле:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha_k \\nabla f(x_k),\n",
    "$$\n",
    "\n",
    "где\n",
    "- $\\nabla f(x_k)$ — градиент функции в точке $x_k$,\n",
    "- $\\alpha_k > 0$ — шаг (скорость обучения), фиксированный или динамический."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c36ffa",
   "metadata": {},
   "source": [
    "Сюда реализацию алгоритма и какой мы берем шаг "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7e5ed",
   "metadata": {},
   "source": [
    "#### **Критерий остановки**\n",
    "Итерации продолжаются до тех пор, пока $ \\|\\nabla f(x_k)\\|_2 $ не станет меньше заданной точности. В нашем случае:\n",
    "$$\n",
    " \\|\\nabla f(x_k)\\|_2 \\leq 0.0001\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
